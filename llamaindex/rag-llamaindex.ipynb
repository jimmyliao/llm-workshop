{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Kernel required: Python 3, ipykernel\n",
    "# !pip install --upgrade pip\n",
    "# !pip install ipykernel\n",
    "# !pip install trulens_eval llama_index html2text\n",
    "# !pip install -r requirements.txt\n",
    "!pip install llama-index llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure OpenAI API\n",
    "import os\n",
    "\n",
    "# load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Azure OpenAI API key\n",
    "AZURE_OPENAI_KEY = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_DEPLOYMENT_EMBEDDING = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "def check_env_var(var_name):\n",
    "    if var_name is None:\n",
    "        print(f\"Please set {var_name} environment variable.\")\n",
    "        exit(1)\n",
    "\n",
    "check_env_var(AZURE_OPENAI_KEY)\n",
    "check_env_var(AZURE_OPENAI_DEPLOYMENT)\n",
    "check_env_var(AZURE_OPENAI_DEPLOYMENT_EMBEDDING)\n",
    "check_env_var(AZURE_OPENAI_ENDPOINT)\n",
    "check_env_var(AZURE_OPENAI_API_VERSION)\n",
    "\n",
    "azure_oai_endpoint = f\"https://{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_OPENAI_API_VERSION}\"\n",
    "\n",
    "print(azure_oai_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘data/llama2.pdf’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "## Setup: Prepare the data\n",
    "!mkdir -p data\n",
    "\n",
    "!wget --no-clobber --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Simple RAG Application, setup loader and document objects\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Azure OpenAI as llm\n",
    "from llama_index.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment=AZURE_OPENAI_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    azure_endpoint=f\"https://{AZURE_OPENAI_ENDPOINT}\",\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "## Setup Index from Nodes VectorStoreIndex\n",
    "## index time varies depending on the number of nodes / documents\n",
    "## For this simple PDF, takes around 10~15 seconds\n",
    "index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "## Setup Query Engine from Index\n",
    "query_engine = index.as_query_engine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of this paper is to explore the AI ethics of ChatGPT through a diagnostic analysis.\n"
     ]
    }
   ],
   "source": [
    "## Send first query to verify the RAG is working\n",
    "query = \"What is the purpose of this paper?\"\n",
    "print(query_engine.query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now you can use above query_engine to query any question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Advanced: Customized RAG Application\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# reuse index or\n",
    "# index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of this paper is to introduce Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. The paper discusses the methods and techniques used to develop these models and emphasizes their alignment with the principles of helpfulness and safety. The authors also highlight the competitiveness of Llama 2 models with existing open-source chat models and their commitment to transparency and ongoing improvements.\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "response = query_engine.query(\"What is the purpose of this paper?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_llm_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
